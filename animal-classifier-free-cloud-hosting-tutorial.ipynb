{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Create, Train and Deploy an image classification model for free - zero hardware","metadata":{}},{"cell_type":"markdown","source":"This notebook will take you through every step needed to create an image classification model. \nWe will be using using Kaggle, Fast.ai HuggingFace, Replit and Github to do everything we need for free! \n\nIt'll be split into 3 main parts:\n1. Getting started and importing dataset.\n2. Creating and training model\n3. Deploying your model\n\nIf you enjoy this notebook, or learn anything. Please upvote and follow me on twitter @jawdinmorris.\nFirst we install the latest version of fastbook:","metadata":{}},{"cell_type":"markdown","source":"# 1.0 Kaggle, Data and Setup","metadata":{}},{"cell_type":"markdown","source":"# 1 Kaggle","metadata":{}},{"cell_type":"markdown","source":"**What is kaggle?** \nKaggle is one of the world's largest data science platforms. It allows you to find, collate and manage datasets, machine learning models and even enter competitions! You're reading this, most likely, on Kaggle right now. \n\n**Why kaggle?**\nBecause it lets us use Jupyter notebooks in the cloud, for free. You can easily create and run code in little Code Blocks. Any code block in this notebook, you can run as you follow along and it will all work. Else, fork your own version and edit whatever you want!\n","metadata":{}},{"cell_type":"markdown","source":"To get your data I recommend using the datasets i've created: \n* https://www.kaggle.com/datasets/jawdinmorris/bird-images\n* https://www.kaggle.com/datasets/jawdinmorris/cat-images\n* https://www.kaggle.com/datasets/jawdinmorris/puppy-images\n\nElse, you need to find and import any datasets you'd like of individual animals. Kaggle has countless other datasets you can use. Our model doesn't need a lot to work, I use about 250 per animal. But meaningful, accurate results can be achieved with less than 50. \n\nYou can use Python and BeautifulSoup, Bing API or even Google Chrome Extensions to quickly gather many images yourself. ","metadata":{}},{"cell_type":"markdown","source":"# 1.2 Setup\nNow you need to setup your kaggle notebook. First, install the latest verison of fastbook Notice the ! at the beginning of the command here, this is to signify it is a bash command, note python code.","metadata":{}},{"cell_type":"code","source":"!pip install -Uqq fastbook","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:33:04.363776Z","iopub.execute_input":"2023-05-15T12:33:04.364335Z","iopub.status.idle":"2023-05-15T12:33:21.447437Z","shell.execute_reply.started":"2023-05-15T12:33:04.364294Z","shell.execute_reply":"2023-05-15T12:33:21.445988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Import fastbook and it's related widgets. \nThen run setup_book to get started.\n\nUnder that is boilerplate code provided by kaggle. This creates links to the data you imported before.","metadata":{}},{"cell_type":"code","source":"import fastbook\nfrom fastbook import *\nfrom fastai.vision.widgets import *\nfastbook.setup_book()\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","scrolled":true,"execution":{"iopub.status.busy":"2023-05-15T12:33:21.454325Z","iopub.execute_input":"2023-05-15T12:33:21.45746Z","iopub.status.idle":"2023-05-15T12:33:21.484268Z","shell.execute_reply.started":"2023-05-15T12:33:21.457417Z","shell.execute_reply":"2023-05-15T12:33:21.483402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Define each of the categories of animal classifications whilst also linking those to the folders the images are nested under. \n\nIn this case we are using animal_types as a list of all our folders containing different species images. ","metadata":{}},{"cell_type":"code","source":"animal_types = 'puppy-images','cat-images', 'bird-images'\npath = Path('/kaggle/input')","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:33:21.489105Z","iopub.execute_input":"2023-05-15T12:33:21.492089Z","iopub.status.idle":"2023-05-15T12:33:21.499142Z","shell.execute_reply.started":"2023-05-15T12:33:21.492056Z","shell.execute_reply":"2023-05-15T12:33:21.498206Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get all the images based on the path defined. Then make sure none of them are corrupted.","metadata":{}},{"cell_type":"code","source":"fns = get_image_files(path)\nfns\nfailed = verify_images(fns)\nfailed","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:33:21.507072Z","iopub.execute_input":"2023-05-15T12:33:21.510066Z","iopub.status.idle":"2023-05-15T12:33:24.26129Z","shell.execute_reply.started":"2023-05-15T12:33:21.510033Z","shell.execute_reply":"2023-05-15T12:33:24.260202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2 Create and Train a model","metadata":{}},{"cell_type":"markdown","source":"# 2.1 Create model","metadata":{}},{"cell_type":"markdown","source":"We now define a DataBlock. A datablock is how Fastbook defines data formats. There are multiple paramaters that can be provided:\n\n* blocks - What style is your input and output?\n    * In this case we are using images as our input and category as our output.\n* get_items - What data are we using?\n    * In this case we defined our image_files above\n* splitter - You must define a validation split for training data\n    * In this case we are using 20% as testing data and setting the seed for that random 20% to be the same every time.\n* get_y - What labels will our data have?\n    * In this case, we're just going to use the parent folder's name\n* item_tfms - We can also transform our data as we put it into the datablock\n    * In this case we're resizing every image to 128px","metadata":{}},{"cell_type":"code","source":"animals = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:33:24.265553Z","iopub.execute_input":"2023-05-15T12:33:24.26983Z","iopub.status.idle":"2023-05-15T12:33:24.279898Z","shell.execute_reply.started":"2023-05-15T12:33:24.269788Z","shell.execute_reply":"2023-05-15T12:33:24.278949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.2 Get the most out of your data\nNow we have the vessel to hold all our data for the model and we have our data, we now need to load that data in using a dataloader. \nWe then show a batch of that data to confirm it's correct.","metadata":{}},{"cell_type":"code","source":"dls = animals.dataloaders(path)\ndls.valid.show_batch(max_n=4, nrows=1)\n","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:33:24.282742Z","iopub.execute_input":"2023-05-15T12:33:24.284413Z","iopub.status.idle":"2023-05-15T12:33:25.318396Z","shell.execute_reply.started":"2023-05-15T12:33:24.284379Z","shell.execute_reply":"2023-05-15T12:33:25.317304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"When we are comparing images to each other, they are compared pixel by pixel so it's important all of our images are the same size. There are multiple methods of resizing, including stretching/squishing, adding borders, or in this case choosing a random area to crop. We do this at a size of 128 - our model really doesn't need large pictures.","metadata":{}},{"cell_type":"code","source":"animals = animals.new(item_tfms=RandomResizedCrop(128, min_scale=0.3))\ndls = animals.dataloaders(path)\ndls.train.show_batch(max_n=4, nrows=1, unique=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:33:25.320323Z","iopub.execute_input":"2023-05-15T12:33:25.321117Z","iopub.status.idle":"2023-05-15T12:33:26.276527Z","shell.execute_reply.started":"2023-05-15T12:33:25.321065Z","shell.execute_reply":"2023-05-15T12:33:26.275299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can also augment your data. As we have 250 images it's actually not needed, but it's a low-cost process and allows us to exponentially increase how much data we have and how 'clean' the data a consumer inputs has to be. ","metadata":{}},{"cell_type":"code","source":"animals = animals.new(item_tfms=Resize(128), batch_tfms=aug_transforms(mult=2))\ndls = animals.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:33:26.278207Z","iopub.execute_input":"2023-05-15T12:33:26.278938Z","iopub.status.idle":"2023-05-15T12:33:28.486215Z","shell.execute_reply.started":"2023-05-15T12:33:26.278901Z","shell.execute_reply":"2023-05-15T12:33:28.485221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"You can loops these together, or set them individually. The below code doesn't do anything new, it's just a different way of showing it.","metadata":{}},{"cell_type":"code","source":"animals = animals.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = animals.dataloaders(path)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:33:28.487963Z","iopub.execute_input":"2023-05-15T12:33:28.488693Z","iopub.status.idle":"2023-05-15T12:33:28.559895Z","shell.execute_reply.started":"2023-05-15T12:33:28.488656Z","shell.execute_reply":"2023-05-15T12:33:28.559018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 2.3 Train a model","metadata":{}},{"cell_type":"markdown","source":"We now have our model set-up, the training is going to take a surprisingly small amount of code due to fast.ai - we already defined some of what we needed to in our Datablock above. Creating a learner and allowing it to finetune is what actually updates the weights and accuracy of our model.\n\nYou pass in at least three parameters:\n* Training data through our dataloader\n** In this case we use dls as we defined above\n* Which method of learning to use\n** In this case we're using resnet34, which is probably a bit overkill for the task at hand. You can find more information about different algos, built on pytorch at the fast.ai docs https://docs.fast.ai/)\n* How we want to measure our success or failure\n** In this case we want to use an error_rate as our metrics\n\nWe then go over and finetune (perform epochs) 20 times.","metadata":{}},{"cell_type":"code","source":"learn = vision_learner(dls, resnet50, metrics=error_rate)\nlearn.fine_tune(5)","metadata":{"execution":{"iopub.status.busy":"2023-05-15T12:33:28.567361Z","iopub.execute_input":"2023-05-15T12:33:28.570965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.0 Deploy the model","metadata":{}},{"cell_type":"code","source":"learn.export()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"From here we move over to huggingface, where we want to take our exported model and upload it to a space. You will need app.py, requirements.txt, your model pkl (named ourModel) and a test image named (dog.jpg). ","metadata":{}},{"cell_type":"markdown","source":"# 3.1 Host the model with HuggingFace\n[Example of my HuggingFace Space](https://huggingface.co/spaces/JawdinMorris/catdog)\nHuggingFace allows us to host the exported model, in conjunction with a framework called Gradio we can make this available to users instantly. Create a HuggingFace Space and create the following files:\n* App.py (Our main code)\n* requirements. (Import needed packages)\n* ourModel.pkl (Our model exported from Kaggle)\n* dog.jpg (A simple example image)","metadata":{}},{"cell_type":"code","source":"## THIS WONT RUN. TAKE THIS TO HUGGINGFACE AS YOUR app.py\n# import gradio as gr\n# from fastai.vision.all import *\n# import skimage\n\n# learn = load_learner('ourModel.pkl')\n\n# labels = learn.dls.vocab\n# def predict(img):\n#     img = PILImage.create(img)\n#     img = img.resize((128,128))\n#     pred,pred_idx,probs = learn.predict(img)\n#     return {labels[i].rstrip('-images'): float(probs[i]) for i in range(len(labels))}\n\n# title = \"Animal Classifier\"\n# description = \"A dog cat classifier test for deployment\"\n# article=\"<p style='text-align: center'>This is our test model</p>\"\n# examples = ['dog.jpg']\n# interpretation='default'\n# enable_queue=True\n\n# gr.Interface(fn=predict,inputs=gr.inputs.Image(shape=(128, 128)),outputs=gr.outputs.Label(num_top_classes=2),title=title,description=description,article=article,examples=examples,interpretation=interpretation,enable_queue=enable_queue).launch()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #THIS WONT RUN. TAKE THIS TO HUGGINGFACE AS requirements.txt\n# fastai\n# scikit-image","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now you actually have a deployed model! You could share the huggingfaces link with friends, or we can use it's easy API to add it wherever we want.","metadata":{}},{"cell_type":"markdown","source":"# 3.2 Code the Website (Replit)","metadata":{}},{"cell_type":"markdown","source":"[Example of Replit Repo](https://replit.com/@JordanMorris5/animal-compare)\n\nUse replit to code the website. Create a HTML, JS, CSS Project. You need three files:\n* index.html\n* script.js\n* style.css\n\nHit the API with https://hf.space/embed/UserName/ProjectName/+/api/predict/\n\nWhere UserName and ProjectName are replaced with your details.\n\nYou can console.log the response to see the output and then assign that as needed. Full template code below.","metadata":{}},{"cell_type":"code","source":"# data Array(1)\n#     confidences Array(3)\n#         0: {label: 'bird', confidence: 0.999995231628418}\n#         1: {label: 'cat', confidence: 0.000004623149834515061}\n#         2:{label: 'puppy', confidence: 1.33642004129797e-7}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ##THIS WILL NOT RUN TAKE IT TO REPLIT AS index.html\n# <!DOCTYPE html>\n# <html>\n\n# <head>\n# \t<meta charset=\"utf-8\">\n# \t<meta name=\"viewport\" content=\"width=device-width\">\n# \t<title>Cat or Puppy?</title>\n# \t<link href=\"style.css\" rel=\"stylesheet\" type=\"text/css\" />\n# </head>\n\n# <body>\n# \t<h1>Cat or Puppy?</h1>\n# \t<div class=\"container\">\n# \t\t<div class=\"input-column\">\n# \t\t\t<h2>Input Photo:</h2>\n# \t\t\t<p>Predict whether an image is of a cat or dog by uploading an image below:</p>\n# \t\t\t<input id=\"photo\" type=\"file\">\n# \t\t\t<br>\n# \t\t\t<button id=\"submit\">Submit</button>\n# \t\t</div>\n# \t\t<div class=\"output-column\">\n# \t\t\t<div id=\"results\"></div>\n# \t\t</div>\n# \t</div>\n# \t<script>\n# \t</script>\n# </body>\n\n# </html>","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#THIS WILL NOT RUN. Take it to replit as script.js\n# \t\tasync function loaded(reader) {\n# \t\t\tconst image = new Image();\n# \t\t\timage.src = reader.result;\n\n# \t\t\tconst canvas = document.createElement('canvas');\n# \t\t\tconst ctx = canvas.getContext('2d');\n# \t\t\tcanvas.width = image.width;\n# \t\t\tcanvas.height = image.height;\n# \t\t\tctx.drawImage(image, 0, 0);\n\n# \t\t\t// Resize the image to a smaller size\n# \t\t\tconst MAX_WIDTH = 128;\n# \t\t\tconst MAX_HEIGHT = 128;\n# \t\t\tlet width = image.width;\n# \t\t\tlet height = image.height;\n\n# \t\t\tif (width > height) {\n# \t\t\t\tif (width > MAX_WIDTH) {\n# \t\t\t\t\theight *= MAX_WIDTH / width;\n# \t\t\t\t\twidth = MAX_WIDTH;\n# \t\t\t\t}\n# \t\t\t} else {\n# \t\t\t\tif (height > MAX_HEIGHT) {\n# \t\t\t\t\twidth *= MAX_HEIGHT / height;\n# \t\t\t\t\theight = MAX_HEIGHT;\n# \t\t\t\t}\n# \t\t\t}\n\n# \t\t\tcanvas.width = width;\n# \t\t\tcanvas.height = height;\n\n# \t\t\tconst resizeCtx = canvas.getContext('2d');\n# \t\t\tresizeCtx.drawImage(image, 0, 0, width, height);\n\n# \t\t\tconst dataURL = canvas.toDataURL('image/jpeg', 0.9);\n\n# \t\t\tconst response = await fetch('https://hf.space/embed/JawdinMorris/catdog/+/api/predict/', {\n# \t\t\t\tmethod: \"POST\", body: JSON.stringify({\"data\": [dataURL]}),\n# \t\t\t\theaders: {\"Content-Type\": \"application/json\"}\n# \t\t\t});\n# \t\t\tconst json = await response.json();\n# \t\t\tconsole.log(json)\n# \t\t\tconst label = json['data'][0]['confidences'][0]['label'];\n# \t\t\tconst confidence = json['data'][0]['confidences'][0]['confidence'] * 100;\n# \t\t\tif (confidence < 95) {\n# \t\t\t\tresults.innerHTML = `<h2>I think you're trying to trick me!</h2>`;\n# \t\t\t} else {\n# \t\t\t\tresults.innerHTML = `<h2> We think this photo is of a...</h2> <p class=\"prediction\">${label} with a ${confidence.toFixed(2)}% confidence</p><img src=\"${dataURL}\" width=\"128\">`\n# \t\t\t}\n\n# \t\t}\n\n# \t\tfunction read() {\n# \t\t\tif (photo.files && photo.files[0]) {\n# \t\t\t\tconst reader = new FileReader();\n\n# \t\t\t\treader.addEventListener('load', () => loaded(reader))\n# \t\t\t\treader.readAsDataURL(photo.files[0]);\n# \t\t\t} else {\n# \t\t\t\t// Handle the case where the file was not uploaded correctly\n# \t\t\t\tconsole.error('No file uploaded');\n# \t\t\t}\n# \t\t}\n# \t\tfunction submitForm() {\n# \t\t\tconst results = document.getElementById('results');\n# \t\t\tresults.innerHTML = `<h2>We're thinking...</h2>`;\n# \t\t\tread();\n# \t\t}\n# \t\tphoto.addEventListener('input', read);\n# \t\tsubmit.addEventListener('click', submitForm);","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#THIS WILL NOT RUN. Take it to replit as styles.css\n# html {\n# \theight: 100%;\n# \twidth: 100%;\n# \tpadding: 0;\n# \tmargin: 0;\n# }\n\n# .container {\n# \tdisplay: flex;\n# \tflex-direction: row;\n# }\n\n# .input-column {\n# \twidth: 40%;\n# \tpadding: 1em;\n# \tdisplay: flex;\n# \tflex-direction: column;\n# }\n\n# .output-column {\n# \twidth: 60%;\n# \tpadding: 1em;\n# \tdisplay: flex;\n# \tflex-direction: column;\n# }\n\n# .prediction {\n# \tfont-size: 2em;\n# }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3.3 Host Website Github\n[Example of my Github Repo](https://github.com/JordanMorrisDev/animal-compare)\n\nFrom there, upload it to Github and create a github pages. It should now be available like here:\nhttps://jordanmorrisdev.github.io/animal-compare/","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}}]}